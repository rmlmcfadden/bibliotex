
@article{2019-Margossian-DMKD-9-e1305,
    author = {Margossian, Charles C.},
    title = {A review of automatic differentiation and its efficient implementation},
    journal = {WIREs Data Min. Knowl. Discovery},
    volume = {9},
    number = {4},
    pages = {e1305},
    keywords = {automatic differentiation, computational statistics, numerical methods},
    doi = {https://doi.org/10.1002/widm.1305},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1305},
    % eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1305},
    abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation (AD) is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of AD, however, requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region-based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open and practical problems include the extension of current packages to provide more specialized routines, and finding optimal methods to perform higher-order differentiation. This article is categorized under: Algorithmic Development > Scalable Statistical Methods},
    year = {2019}
}

@article{2020-Gebremedhin-DMKD-10-e1334,
    author = {Gebremedhin, Assefaw H. and Walther, Andrea},
    title = {An introduction to algorithmic differentiation},
    journal = {WIREs Data Min. Knowl. Discovery},
    volume = {10},
    number = {1},
    pages = {e1334},
    keywords = {adjoints, algorithmic differentiation, automatic differentiation, backpropagation, checkpointing, sensitivities},
    doi = {https://doi.org/10.1002/widm.1334},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1334},
    % eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1334},
    abstract = {Abstract Algorithmic differentiation (AD), also known as automatic differentiation, is a technology for accurate and efficient evaluation of derivatives of a function given as a computer model. The evaluations of such models are essential building blocks in numerous scientific computing and data analysis applications, including optimization, parameter identification, sensitivity analysis, uncertainty quantification, nonlinear equation solving, and integration of differential equations. We provide an introduction to AD and present its basic ideas and techniques, some of its most important results, the implementation paradigms it relies on, the connection it has to other domains including machine learning and parallel computing, and a few of the major open problems in the area. Topics we discuss include: forward mode and reverse mode of AD, higher-order derivatives, operator overloading and source transformation, sparsity exploitation, checkpointing, cross-country mode, and differentiating iterative processes. This article is categorized under: Algorithmic Development > Scalable Statistical Methods Technologies > Data Preprocessing},
    year = {2020}
}

